# -*- coding: utf-8 -*-
"""SVHN_Digit_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pcrSeEM5zUN_bFhqSIlZGzNS3ZgtxxpT

# **Deep Learning Project: Street View Housing Number Digit Recognition**

# **Marks: 60**


--------------
## **Context**
--------------

One of the most interesting tasks in deep learning is to recognize objects in natural scenes. The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications.

The SVHN dataset contains over 600,000 labeled digits cropped from street-level photos. It is one of the most popular image recognition datasets. It has been used in neural networks created by Google to improve the map quality by automatically transcribing the address numbers from a patch of pixels. The transcribed number with a known street address helps pinpoint the location of the building it represents.

----------------
## **Objective**
----------------

Our objective is to predict the number depicted inside the image by using Artificial or Fully Connected Feed Forward Neural Networks and Convolutional Neural Networks. We will go through various models of each and finally select the one that is giving us the best performance.

-------------
## **Dataset**
-------------
Here, we will use a subset of the original data to save some computation time. The dataset is provided as a .h5 file. The basic preprocessing steps have been applied on the dataset.

### **Please read the instructions carefully before starting the project.**

This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned. Read along carefully to complete the project.

* Blanks '_______' are provided in the notebook that needs to be filled with an appropriate code to get the correct result. Please replace the blank with the right code snippet. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.
* Identify the task to be performed correctly, and only then proceed to write the required code.
* Fill the code wherever asked by the commented lines like "# Fill in the blank" or "# Complete the code". Running incomplete code may throw an error.
* Remove the blank and state your observations in detail wherever the mark down says 'Write your observations here:_________'
* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.
* You can the results/observations derived from the analysis here and use them to create your final report.

## **Mount the drive**

Let us start by mounting the Google drive. You can run the below cell to mount the Google drive.
"""

from google.colab import drive

drive.mount('/content/drive')

"""## **Importing the necessary libraries**"""

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization

from tensorflow.keras.losses import categorical_crossentropy

from tensorflow.keras.optimizers import Adam

from tensorflow.keras.utils import to_categorical

"""**Let us check the version of tensorflow.**"""

print(tf.__version__)

"""## **Load the dataset**

- Let us now load the dataset that is available as a .h5 file.
- Split the data into the train and the test dataset.
"""

import h5py

# Open the file as read only
# User can make changes in the path as required

h5f = h5py.File('/content/drive/MyDrive/MIT Professional Education: Applied Data Science/Week 10-12 Capstone Project/SVHN_single_grey1.h5', 'r')

# Load the training and the test dataset

X_train = h5f['X_train'][:]

y_train = h5f['y_train'][:]

X_test = h5f['X_test'][:]

y_test = h5f['y_test'][:]


# Close this file

h5f.close()

"""Let's check the number of images in the training and the testing dataset."""

len(X_train), len(X_test)

len(y_train), len(y_test)

X_train.shape, X_test.shape

"""**Observation:**
- There are 42,000 images in the training data and 18,000 images in the testing data.

## **Visualizing images**

- Use X_train to visualize the first 10 images.
- Use Y_train to print the first 10 labels.
"""

# Visualizing the first 10 images in the dataset and printing their labels

plt.figure(figsize = (10, 1))

for i in range(10):

    plt.subplot(1, 10, i+1)

    plt.imshow(X_train[i], cmap = "gray")

    plt.axis('off')

plt.show()

print('label for each of the above image: %s' % (y_train[0:10]))

"""## **Data preparation**

- Print the shape and the array of pixels for the first image in the training dataset.
- Normalize the train and the test dataset by dividing by 255.
- Print the new shapes of the train and the test dataset.
- One-hot encode the target variable.
"""

# Shape and the array of pixels for the first image

print("Shape:", X_train[0].shape)

print()

print("First image:\n", X_train[0])

"""* Reshape the dataset to flatten them
* Needs **verification** on the difference between flattening the data through reshaping during the pre-processing steps versus flattening the data as a hidden layer **!!!!**
"""

# Reshaping the dataset to flatten them. We are reshaping the 2D image into 1D array

X_train = X_train.reshape(X_train.shape[0], 1024)

X_test = X_test.reshape(X_test.shape[0], 1024)

"""### **Normalize the train and the test data.**"""

# Normalize inputs from 0-255 to 0-1

X_train = X_train/255

X_test = X_test/255

print(X_train)
print(X_test)

# New shape

print('Training set:', X_train.shape, y_train.shape)

print('Test set:', X_test.shape, y_test.shape)

np.unique(y_train)

"""**One-hot encode**

Converts a class vector (integers) to binary class matrix.

- We need to one hot encode the target variable to be able to form the training target vector.
- Hint: check tf.keras.utils.to_categorical() - https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical
"""

# One-hot encode output

y_train = to_categorical(y_train)

y_test = to_categorical(y_test)

# Test labels

y_test

"""**Observation:**
- Notice that each entry of the target variable is a one-hot encoded vector instead of a single label.
"""

X_train.shape, y_train.shape, X_test.shape, y_test.shape

"""## **Model Building**

Now that we have done the data preprocessing, let's build an ANN model.
"""

# Fixing the seed for random number generators

np.random.seed(42)

import random

random.seed(42)

tf.random.set_seed(42)

"""### **Model Architecture**
- Write a function that returns a sequential model with the following architecture:
 - First hidden layer with **64 nodes and the relu activation** and the **input shape = (1024, )**
 - Second hidden layer with **32 nodes and the relu activation**
 - Output layer with **activation as 'softmax' and number of nodes equal to the number of classes, i.e., 10**
 - Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.
- Call the nn_model_1 function and store the model in a new variable.
- Print the summary of the model.
- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 20**. Store the model building history to use later for visualization.

### **Build and train an ANN model as per the above mentioned architecture.**
"""

# Define the model

def nn_model_1():

    model = Sequential()

    # Add layers as per the architecture mentioned above in the same sequence

    model.add(tf.keras.layers.Dense(64, input_shape = (1024,), activation= 'relu'))

    model.add(tf.keras.layers.Dense(32, activation= 'relu'))

    model.add(tf.keras.layers.Dense(10, activation = 'softmax'))

    # Compile the model

    opt = tf.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer = opt, loss = 'categorical_crossentropy',  metrics = ['accuracy'])

    return model

# Build the model

model_1 = nn_model_1()

# Print the summary

model_1.summary()

# Fit the model

history_model_1 = model_1.fit(X_train, y_train, validation_split = 0.2, verbose = 1, epochs = 20, batch_size = 128)

"""### **Plotting the validation and training accuracies**

### **Write your observations on the below plot.**
"""

# Plotting the accuracies

dict_hist = history_model_1.history

list_ep = [i for i in range(1, 21)]

plt.figure(figsize = (8, 8))

plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy')

plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy')

plt.ylabel('Accuracy')

plt.xlabel('Epochs')

plt.legend()

plt.show()

"""**Observations:_______**

Let's build one more model with higher complexity and see if we can improve the performance of the model.

First, we need to clear the previous model's history from the Keras backend. Also, let's fix the seed again after clearing the backend.
"""

# Clearing backend

from tensorflow.keras import backend

backend.clear_session()

# Fixing the seed for random number generators

np.random.seed(42)

import random

random.seed(42)

tf.random.set_seed(42)

"""### **Second Model Architecture**
- Write a function that returns a sequential model with the following architecture:
 - First hidden layer with **256 nodes and the relu activation** and the **input shape = (1024, )**
 - Second hidden layer with **128 nodes and the relu activation**
 - Add the **Dropout layer with the rate equal to 0.2**
 - Third hidden layer with **64 nodes and the relu activation**
 - Fourth hidden layer with **64 nodes and the relu activation**
 - Fifth hidden layer with **32 nodes and the relu activation**
 - Add the **BatchNormalization layer**
 - Output layer with **activation as 'softmax' and number of nodes equal to the number of classes, i.e., 10**
 -Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.0005), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.
- Call the nn_model_2 function and store the model in a new variable.
- Print the summary of the model.
- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 30**. Store the model building history to use later for visualization.

### **Build and train the new ANN model as per the above mentioned architecture.**
"""

# Define the model

def nn_model_2():

    model = Sequential()

    # Add layers as per the architecture mentioned above in the same sequence

    model.add(tf.keras.layers.Dense(256, input_shape = (1024,), activation= 'relu'))

    model.add(tf.keras.layers.Dense(128, activation= 'relu'))

    model.add(tf.keras.layers.Dropout(0.2)) #dropout layer (rate = 0.2)

    model.add(tf.keras.layers.Dense(64, activation= 'relu'))

    model.add(tf.keras.layers.Dense(64, activation= 'relu'))

    model.add(tf.keras.layers.Dense(32, activation= 'relu'))

    model.add(tf.keras.layers.BatchNormalization()) #BatchNormalization layer

    model.add(tf.keras.layers.Dense(10, activation = 'softmax'))

    # Compile the model

    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005), loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model

# Build the model

model_2 = nn_model_2()

# Print the model summary
model_2.summary()

# Fit the model

history_model_2 = model_2.fit(X_train, y_train, epochs = 30, validation_split = 0.2, batch_size = 128, verbose = 1)

"""### **Plotting the validation and training accuracies**

### **Write your observations on the below plot.**
"""

# Plotting the accuracies

dict_hist = history_model_2.history

list_ep = [i for i in range(1, 31)]

plt.figure(figsize = (8, 8))

plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy')

plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy')

plt.ylabel('Accuracy')

plt.xlabel('Epochs')

plt.legend()

plt.show()

"""**Observations:_______**

## **Predictions on the test data**

- Make predictions on the test set using the second model.
- Print the obtained results using the classification report and the confusion matrix.
- Final observations on the obtained results.
"""

test_pred = model_2.predict(X_test)

test_pred = np.argmax(test_pred, axis = -1)

"""**Note:** Earlier, we noticed that each entry of the target variable is a one-hot encoded vector but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label."""

# Converting each entry to single label from one-hot encoded vector

y_test = np.argmax(y_test, axis = -1)

"""### **Print the classification report and the confusion matrix for the test predictions. Write your observations on the final results.**"""

# Importing required functions

from sklearn.metrics import classification_report

from sklearn.metrics import confusion_matrix

# Printing the classification report

print(classification_report(y_test, test_pred, digits=4))

# Plotting the heatmap using confusion matrix

cm = confusion_matrix(y_test, test_pred) #Write the code for creating confusion matrix using actual labels (y_test) and predicted labels (test_pred)

plt.figure(figsize = (8, 5))

sns.heatmap(cm, annot = True,  fmt = '.0f')

plt.ylabel('Actual')

plt.xlabel('Predicted')

plt.show()

"""**Final Observations:__________**

## **Using Convolutional Neural Networks**
"""

# Clearing backend

from tensorflow.keras import backend

backend.clear_session()

import h5py

# Open the file as read only
# User can make changes in the path as required

h5f = h5py.File('/content/drive/MyDrive/MIT Professional Education: Applied Data Science/Week 10-12 Capstone Project/SVHN_single_grey1.h5', 'r')

X_train = h5f['X_train'][:]

y_train = h5f['y_train'][:]

X_test = h5f['X_test'][:]

y_test = h5f['y_test'][:]


# Close this file

h5f.close()

"""Let's check the number of images in the training and the testing dataset."""

len(X_train), len(X_test)

"""**Observation:**
- There are 42,000 images in the training data and 18,000 images in the testing data.

## **Data preparation**

- Print the shape and the array of pixels for the first image in the training dataset.
- Reshape the train and the test dataset because we always have to give a 4D array as input to CNNs.
- Normalize the train and the test dataset by dividing by 255.
- Print the new shapes of the train and the test dataset.
- One-hot encode the target variable.
"""

# Shape and the array of pixels for the first image

print("Shape:", X_train[0].shape)

print()

print("First image:\n", X_train[0])

# Reshaping the dataset to be able to pass them to CNNs. Remember that we always have to give a 4D array as input to CNNs

X_train = X_train.reshape(X_train.shape[0], 32, 32, 1)

X_test = X_test.reshape(X_test.shape[0], 32, 32, 1)

# Normalize inputs from 0-255 to 0-1

X_train = X_train / 255.0

X_test = X_test / 255.0

# New shape

print('Training set:', X_train.shape, y_train.shape)

print('Test set:', X_test.shape, y_test.shape)

"""### **One-hot encode the labels in the target variable y_train and y_test.**"""

# Write the function and appropriate variable name to one-hot encode the output

y_train = to_categorical(y_train)

y_test = to_categorical(y_test)

# test labels

y_test

"""**Observation:**
- Notice that each entry of the target variable is a one-hot encoded vector instead of a single label.

## **Model Building**

Now that we have done data preprocessing, let's build a CNN model.
"""

# Fixing the seed for random number generators

np.random.seed(42)

import random

random.seed(42)

tf.random.set_seed(42)

"""### **Model Architecture**
- **Write a function** that returns a sequential model with the following architecture:
 1. First Convolutional layer with **16 filters and the kernel size of 3x3**. Use the **'same' padding** and provide the **input shape = (32, 32, 1)**
 2. Add a **LeakyRelu layer** with the **slope equal to 0.1**
 3. Second Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**
 4. Another **LeakyRelu** with the **slope equal to 0.1**
 5. A **max-pooling layer** with a **pool size of 2x2**
 6. **Flatten** the output from the previous layer
 7. Add a **dense layer with 32 nodes**
 8. Add a **LeakyRelu layer with the slope equal to 0.1**
 9. Add the final **output layer with nodes equal to the number of classes, i.e., 10** and **'softmax' as the activation function**
 * Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.
- Call the function cnn_model_1 and store the output in a new variable.
- Print the summary of the model.
- Fit the model on the training data with a **validation split of 0.2, batch size = 32, verbose = 1, and epochs = 20**. Store the model building history to use later for visualization.

### **Build and train a CNN model as per the above mentioned architecture.**
"""

X_test.shape

#important important layers in CNN
from tensorflow.keras.layers import Conv2D, LeakyReLU, MaxPooling2D, Flatten

# Define the model

def cnn_model_1():

    model = Sequential()

    # Add 10 layers as per the architecture mentioned above in the same sequence (9 featured learning layers and 1 output layer)

    model.add(Conv2D(16, (3, 3), activation='relu', padding="same", input_shape=(32, 32, 1)))

    model.add(LeakyReLU(alpha=0.01))

    model.add(Conv2D(32, (3, 3), activation='relu', padding="same"))

    model.add(LeakyReLU(alpha=0.01))

    model.add(MaxPooling2D((2, 2), padding = 'same'))

    model.add(Flatten())

    model.add(Dense(32, activation='relu'))

    model.add(LeakyReLU(alpha=0.01))

    model.add(Dense(10, activation='softmax'))

    # Compile the model

    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model

# Build the model

cnn_model_1 = cnn_model_1()

# Print the model summary

cnn_model_1.summary()

# Fit the model

history_cnn_model_1 = cnn_model_1.fit(X_train, y_train, epochs = 20, validation_split = 0.2, batch_size = 32, verbose = 1)

"""### **Plotting the validation and training accuracies**

### **Write your observations on the below plot.**
"""

# Plotting the accuracies

dict_hist = history_cnn_model_1.history

list_ep = [i for i in range(1, 21)]

plt.figure(figsize = (8, 8))

plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy')

plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy')

plt.ylabel('Accuracy')

plt.xlabel('Epochs')

plt.legend()

plt.show()

"""**Observations:__________**

Let's build another model and see if we can get a better model with generalized performance.

First, we need to clear the previous model's history from the Keras backend. Also, let's fix the seed again after clearing the backend.
"""

# Clearing backend

from tensorflow.keras import backend

backend.clear_session()

# Fixing the seed for random number generators

np.random.seed(42)

import random

random.seed(42)

tf.random.set_seed(42)

"""### **Second Model Architecture**

- Write a function that returns a sequential model with the following architecture:
 - First Convolutional layer with **16 filters and the kernel size of 3x3**. Use the **'same' padding** and provide the **input shape = (32, 32, 1)**
 - Add a **LeakyRelu layer** with the **slope equal to 0.1**
 - Second Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**
 - Add **LeakyRelu** with the **slope equal to 0.1**
 - Add a **max-pooling layer** with a **pool size of 2x2**
 - Add a **BatchNormalization layer**
 - Third Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**
 - Add a **LeakyRelu layer with the slope equal to 0.1**
 - Fourth Convolutional layer **64 filters and the kernel size of 3x3 with 'same' padding**
 - Add a **LeakyRelu layer with the slope equal to 0.1**
 - Add a **max-pooling layer** with a **pool size of 2x2**
 - Add a **BatchNormalization layer**
 - **Flatten** the output from the previous layer
 - Add a **dense layer with 32 nodes**
 - Add a **LeakyRelu layer with the slope equal to 0.1**
 - Add a **dropout layer with the rate equal to 0.5**
 - Add the final **output layer with nodes equal to the number of classes, i.e., 10** and **'softmax' as the activation function**
 - Compile the model with the **categorical_crossentropy loss, adam optimizers (learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.
- Call the function cnn_model_2 and store the model in a new variable.
- Print the summary of the model.
- Fit the model on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 30**. Store the model building history to use later for visualization.

### **Build and train the second CNN model as per the above mentioned architecture.**
"""

# Define the model

def cnn_model_2():

    model = Sequential()

    # Add layers as per the architecture mentioned above in the same sequence

    model.add(Conv2D(16, (3, 3), activation='relu', padding="same", input_shape=(32, 32, 1)))

    model.add(LeakyReLU(alpha=0.01))

    model.add(Conv2D(32, (3, 3), activation='relu', padding="same"))

    model.add(LeakyReLU(alpha=0.01))

    model.add(MaxPooling2D((2, 2), padding = 'same'))

    model.add(tf.keras.layers.BatchNormalization()) #Batch Normalization

    model.add(Conv2D(32, (3, 3), activation='relu', padding="same"))

    model.add(LeakyReLU(alpha=0.01))

    model.add(Conv2D(64, (3, 3), activation='relu', padding="same"))

    model.add(LeakyReLU(alpha=0.01))

    model.add(MaxPooling2D((2, 2), padding = 'same'))

    model.add(tf.keras.layers.BatchNormalization()) #Batch Normalization

    model.add(Flatten()) #Flatten the output from the previous layer

    model.add(Dense(32, activation='relu'))

    model.add(LeakyReLU(alpha=0.01))

    model.add(tf.keras.layers.Dropout(0.5)) #Drop out layer with rate = 0.5)

    model.add(Dense(10, activation='softmax'))

    # Compile the model

    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model

# Build the model

cnn_model_2 = cnn_model_2()

# Print the summary

cnn_model_2.summary()

# Fit the model

history_cnn_model_2 = cnn_model_2.fit(X_train, y_train, epochs = 30, validation_split = 0.2, batch_size = 128, verbose = 1)

"""### **Plotting the validation and training accuracies**

### **Write your observations on the below plot**
"""

# Plotting the accuracies

dict_hist = history_cnn_model_2.history

list_ep = [i for i in range(1, 31)]

plt.figure(figsize = (8, 8))

plt.plot(list_ep, dict_hist['accuracy'], ls = '--', label = 'accuracy')

plt.plot(list_ep, dict_hist['val_accuracy'], ls = '--', label = 'val_accuracy')

plt.ylabel('Accuracy')

plt.xlabel('Epochs')

plt.legend()

plt.show()

"""**Observations:________**

## **Predictions on the test data**

- Make predictions on the test set using the second model.
- Print the obtained results using the classification report and the confusion matrix.
- Final observations on the obtained results.

### **Make predictions on the test data using the second model.**
"""

# Make prediction on the test data using model_2

test_pred = cnn_model_2.predict(X_test)

test_pred = np.argmax(test_pred, axis = -1)

"""**Note:** Earlier, we noticed that each entry of the target variable is a one-hot encoded vector, but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label."""

# Converting each entry to single label from one-hot encoded vector

y_test = np.argmax(y_test, axis = -1)

"""### **Write your final observations on the performance of the model on the test data.**"""

# Importing required functions

from sklearn.metrics import classification_report

from sklearn.metrics import confusion_matrix

# Printing the classification report

print(classification_report(y_test, test_pred))

# Plotting the heatmap using confusion matrix

cm = confusion_matrix(y_test, test_pred)

plt.figure(figsize = (8, 5))

sns.heatmap(cm, annot = True,  fmt = '.0f')

plt.ylabel('Actual')

plt.xlabel('Predicted')

plt.show()

"""**Final Observations:_________**"""
